---
title: "002-分布式事务.md"
date: 2025-12-28 15:45:36
tags: []
---


### 1. 为什么会有分布式事务问题？

原因：
- 微服务拆分
- 多数据库
- 跨系统调用

---

### 2. CAP 理论是什么？

- **Consistency（一致性）**
- **Availability（可用性）**
- **Partition tolerance（分区容错）**

分布式系统必须满足 P，只能在 C 和 A 之间取舍。

---

### 3. 强一致性和最终一致性区别？

- 强一致性：数据实时一致（代价高）
- 最终一致性：允许短暂不一致（主流方案）

---

### 4. 常见分布式事务方案？

| 分布式事务方案 | 一致性保证 | 性能损耗 | 实现复杂度 | 适用场景 |
|----------------|------------|----------|------------|----------|
| 2PC（两阶段提交） | 强一致性 | 高（阻塞） | 中 | 传统金融、低并发核心业务 |
| TCC（补偿事务） | 强一致性 | 低（非阻塞） | 高 | 高并发核心业务（如下单扣库存） |
| SAGA | 最终一致性 | 低 | 中 | 长流程业务（如订单履约、物流） |
| 本地消息表 | 最终一致性 | 低 | 低 | 中小型系统、异步通知场景 |
| 可靠消息队列（MQ） | 最终一致性 | 低 | 中 | 高并发异步场景（如电商通知） |

---

### 5. 2PC 是什么？

#### 2PC 事务核心原理
2PC（Two-Phase Commit）是分布式事务的 **经典强一致性方案**，引入 **事务协调者（TC）** 角色统一管控流程，将分布式事务拆分为 **两个核心阶段**：

#### 1. 准备阶段（Prepare）
1.  事务协调者向所有参与者（如订单服务、库存服务、支付服务）发送 **准备执行请求**；
2.  各参与者执行本地事务操作（如扣减库存、生成订单记录），但**不提交事务**，仅记录 `undo`/`redo` 日志（用于后续提交或回滚）；
3.  参与者执行完毕后，向协调者返回执行结果：**就绪（Ready）** 表示本地操作成功，**失败（Abort）** 表示本地操作失败。

#### 2. 提交阶段（Commit）
该阶段的执行逻辑完全取决于准备阶段所有参与者的反馈结果，分为两种情况：
- **情况 1：所有参与者均返回就绪**
    1.  协调者向所有参与者发送 **全局提交请求**；
    2.  各参与者执行本地事务提交操作，释放事务过程中锁定的资源；
    3.  参与者向协调者反馈提交成功，全局事务完成。

- **情况 2：任一参与者返回失败**
    1.  协调者向所有参与者发送 **全局回滚请求**；
    2.  各参与者根据之前记录的 `undo` 日志，执行本地事务回滚操作，恢复到事务执行前的状态；
    3.  参与者向协调者反馈回滚成功，全局事务终止。

#### 2PC 事务的优缺点
| 维度 | 具体说明 |
|------|----------|
| **优点** | 1.  **强一致性**：理论上严格保证分布式事务的原子性、一致性，满足 ACID 特性；<br>2.  **实现简单**：流程逻辑清晰，协调者与参与者的交互规则易于理解和实现。 |
| **缺点** | 1.  **阻塞问题严重**：准备阶段后，参与者会锁定资源，直到收到协调者的提交/回滚指令，高并发场景下会导致资源长时间占用，性能急剧下降；<br>2.  **单点故障风险**：协调者是核心瓶颈，若协调者在提交阶段宕机，参与者会一直处于阻塞状态，资源无法释放；<br>3.  **脑裂风险**：协调者发送提交指令后宕机，部分参与者收到指令并提交，部分未收到，最终导致数据不一致。 |
---

### 6. TCC 的核心思想？

#### 事务核心原理
TCC 是 **补偿型事务方案**，不依赖数据库事务，通过 **业务层代码** 实现分布式事务的原子性，分为 **3 个核心阶段**：

#### 1. Try（尝试）
- 执行 **资源检查 + 资源预留**，例如：检查库存是否充足、预扣商品库存、冻结用户账户余额；
- 关键要求：Try 操作必须是 **幂等的**（重复调用结果一致），且 **可撤销**（支持后续 Cancel 操作回滚）。

#### 2. Confirm（确认）
- 触发条件：当所有参与者的 Try 阶段都执行成功后，全局事务协调器发起 Confirm 操作；
- 核心动作：真正执行业务逻辑，例如：确认扣减预占库存、确认生成订单、确认扣除冻结余额；
- 关键要求：Confirm 操作必须是 **幂等的**，且 **不允许失败**（需通过重试、日志兜底等手段保证 100% 执行成功）。

#### 3. Cancel（取消）
- 触发条件：当任一参与者的 Try 阶段执行失败后，全局事务协调器发起 Cancel 操作；
- 核心动作：撤销 Try 阶段的资源预留，恢复业务状态，例如：释放预扣的库存、解冻用户账户余额；
- 关键要求：Cancel 操作同样必须是 **幂等的**。

> ⚠️ **核心特点：对业务侵入强**
> TCC 方案需要深度耦合业务逻辑，每个分布式事务参与者都需实现 Try/Confirm/Cancel 三个接口。

#### TCC 事务的优缺点
| 维度 | 具体说明 |
|------|----------|
| **优点** | 1. 性能高：非阻塞设计，Try 阶段仅预留资源，不会长期锁定数据库等核心资源；<br>2. 灵活性强：完全由业务代码控制流程，适配各种复杂业务场景；<br>3. 高并发友好：无单点瓶颈，支持服务水平扩容。 |
| **缺点** | 1. 开发成本高：每个业务都需要编写 Try/Confirm/Cancel 三个方法，代码量翻倍；<br>2. 业务侵入性强：需改造现有业务逻辑，设计合理的资源预留模型（如预扣库存、冻结余额）；<br>3. 容错成本高：需手动处理 Confirm/Cancel 阶段的重试、幂等、异常兜底等问题。 |

---

### 7. Saga 模式是什么？

#### 核心原理
SAGA 是 **基于补偿的最终一致性方案**，将分布式事务拆分为 **一系列独立的本地事务**，每个本地事务都有对应的 **补偿事务**：
- **正向流程**：按顺序执行 T1 → T2 → T3 → … → Tn 本地事务；
- **补偿流程**：若某个 Ti 执行失败，按逆序执行补偿事务 Cn → … → C2 → C1，撤销已执行的本地事务。

#### 两种执行策略
| 策略 | 适用场景 | 特点 |
|-----|----------|------|
| 编排式 | 简单流程 | 无中心协调者，各服务通过事件通知触发下一个服务，如 T1 执行完发送事件触发 T2 |
| 协调式 | 复杂流程 | 引入中心协调者（Saga Coordinator），由协调者统一调度正向 / 补偿流程，如借助 Camunda 框架 |


#### 优点
1.  适合 **长流程业务**：拆分多个步骤，支持跨天 / 跨系统的事务（如订单履约、物流配送）；
2.  **性能高**：无阻塞，每个本地事务独立提交，不会像 2PC 那样长时间占用资源；
3.  **实现复杂度低于 TCC**：无需为每个业务方法编写 Try/Confirm/Cancel 三个接口，仅需实现正向和补偿逻辑。

#### 缺点
1.  仅保证 **最终一致性**：在补偿流程执行完成前，数据会处于临时不一致状态（如订单已创建但库存未扣减）；
2.  **补偿事务设计复杂**：需考虑幂等性（避免重复补偿）、重试机制（应对补偿失败）、业务回滚边界（如已发货的订单无法直接补偿）。

---

### 8. 基于 MQ 的最终一致性方案（重点）

核心流程：
1. 本地事务
2. 发送 MQ 消息
3. 消费端执行
4. 失败重试 / 补偿

关键点：
- 消息不丢
- 消费幂等

---

### 9. 本地事务 + MQ 如何保证一致？

常见做法：
- 本地事务 + MQ 事务消息
- 本地消息表（Outbox）

---

### 10. 什么是“本地消息表”？

- 业务表 + 消息表
- 同一事务提交
- 定时投递 MQ

---

### 11. 分布式事务失败如何补偿？

- 重试
- 人工兜底
- 补偿接口

---

### 12. 雪花算法时钟回拨怎么解决？

服务器时钟回拨（比如从 1000ms 回拨到 990ms），会导致新生成的 ID 时间戳位小于历史 ID，甚至重复。

| 异常类型       | 现象                                       | 风险                                                         |
|----------------|--------------------------------------------|--------------------------------------------------------------|
| 时钟小幅回拨   | 回拨时间 < 序列号最大容忍范围（如 5ms）| 同一毫秒内序列号耗尽，可能生成重复 ID                         |
| 时钟大幅回拨   | 回拨时间 > 序列号最大容忍范围（如 100ms）| 新 ID 时间戳小于历史 ID，破坏递增性，甚至重复                 |
| 时钟跳变（超前）| 时间突然跳快（如从 1000ms 跳到 2000ms）| ID 时间戳失真，但不会重复（影响较小）|

#### 1. 基础方案：时钟回拨检测 + 等待 / 顺延时间（核心）

这是所有雪花算法优化版本的基础逻辑，核心是「检测到时间回拨后，不立即生成 ID，而是等待时钟恢复正常，或顺延时间戳」

- 核心逻辑拆解：
  - **时钟回拨检测**：每次生成 ID 时，对比当前时间戳和上次生成 ID 的时间戳；
  - **小幅回拨处理**：若回拨时间短（如 <5ms），等待时钟恢复到「上次时间戳 + 1ms」，避免重复；
  - **大幅回拨处理**：直接抛异常（或降级为其他 ID 生成方案），拒绝生成无效 ID；
  - **序列号兜底**：同一毫秒内序列号耗尽时，等待到下一个毫秒，保证 ID 唯一性。

#### 2. 进阶方案：引入物理时钟 / 分布式时间服务（解决大幅回拨）

对于要求极高的分布式场景（如金融、电商），仅靠本地时钟检测不够，可结合外部时间源：

- 方案 1：**对接 NTP 服务器**：定期同步网络时间，检测到本地时钟偏差过大时，自动校准（需保证校准过程平滑，避免突然回拨）；
- 方案 2：**分布式时间服务**：集群内选举一个「时间主节点」，所有节点从主节点获取时间戳生成 ID，避免单节点时钟异常；
- 方案 3：**引入物理时钟**：服务器挂载高精度时钟设备（如 GPS 时钟），降低本地时钟异常概率。

#### 3. 终极方案：重设计时间戳位（容忍时钟回拨）

对雪花算法的结构做微调，从根本上降低时间异常的影响：

- 方案 1：**时间戳位 + 偏移量**：生成 ID 时，时间戳不是「当前时间 - 起始时间」，而是「当前时间 - 起始时间 + 历史最大偏移量」，确保时间戳只增不减；
- 方案 2：**混合逻辑时钟**：将物理时间和逻辑计数结合（比如时间戳位 = 物理时间 + 逻辑回拨次数），即使物理时钟回拨，逻辑计数递增仍能保证 ID 唯一；
- 方案 3：**改用纳秒级时间戳**：将时间戳精度从毫秒提升到纳秒，减少同一时间单位内的序列号竞争，降低因序列号耗尽导致的 ID 生成失败概率。

#### 4. 兜底方案：降级机制（避免服务不可用）
若检测到严重的时间异常（如回拨超 100ms），可触发降级：
- 降级到备用 ID 生成方案：比如改用 UUID（牺牲递增性，保证唯一性）；
- 降级到本地序列号：暂时用「机器位 + 本地自增序列」生成 ID，待时钟恢复后切回雪花算法；
- 熔断保护：短时间内拒绝生成 ID，避免大量重复 / 无效 ID 产生。

#### 成熟实现的参考（避免重复造轮子）
生产环境无需自己实现复杂的时间异常处理，优先用成熟的雪花算法封装库：
- Twitter 官方 Snowflake：基础版本，需自己扩展时钟回拨检测；
- 美团 Leaf：内置「时钟回拨检测 + 等待顺延」机制，支持分布式部署；
- 百度 UidGenerator：支持「时间回拨容忍」，可配置时间回拨阈值，序列号耗尽时自动顺延时间；
- Hutool Snowflake：轻量级实现，内置简单的时钟回拨检测，适合中小项目。

---





