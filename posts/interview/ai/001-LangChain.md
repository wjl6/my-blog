---
title: "001-LangChain.md"
date: 2025-12-26 14:41:08
tags: []
---

## 一、LangChain 基础认知（必问）

### 1. 什么是 LangChain？它解决了什么问题？

LangChain 是一个 **LLM 应用开发框架**，核心目标是：
- 简化复杂 LLM 应用的构建流程
- 将 LLM 从“聊天工具”变成“可编排、可控制、可工程化的能力模块”

#### 解决的问题：
- LLM 缺乏外部知识，易产生 “幻觉”
> 检索增强生成（RAG）组件
- LLM 无法持久化记忆，对话无上下文
> 记忆（Memory）组件
- LM 无法调用外部工具，能力受限
> 工具（Tools）与代理（Agent）组件
- 复杂 LLM 应用开发流程繁琐，重复造轮子
> 链（Chain）机制
- LLM 应用的调试与评估困难
> 可观测性与评估工具（LangSmith）

---

### 2. LangChain 的核心组件有哪些？

- Models（模型）
- Prompts（提示词）
- Chains（链）
- Memory（记忆）
- Agents（智能体）
- Retrievers / VectorStores（检索与向量存储）

---

### 3. LangChain 和直接调用大模型 API 有什么区别？

- 直接调用大模型 API 适合简单、一次性请求；
- LangChain 适合复杂、可组合、可维护的 LLM 应用系统。

> LangChain 并不是替代模型 API，而是在模型之上提供了一层“AI 应用编排框架”。

| 维度         | 直接调用大模型 API | LangChain |
|--------------|--------------------|-----------|
| 本质         | 大模型的原生接口（如 OpenAI /chat/completions），仅负责 “输入文本→输出文本” | 大模型应用开发框架，封装 API 并扩展落地能力，聚焦 “如何用大模型解决业务问题” |
| 核心目标     | 完成单次文本生成 / 对话 | 构建端到端的大模型应用（如 RAG、智能体、多轮对话机器人） |
| 抽象层级     | 底层（原子操作） | 上层（应用级操作） |
| 依赖关系     | 无需额外依赖，直接调接口 | 依赖大模型 API，是对 API 的二次封装 |

---

## 二、RAG（检索增强生成，重点）

### 1. 什么是 RAG？为什么必须用？

RAG = **检索 + 生成**

解决：
- 模型幻觉
- 私有知识无法回答
- 数据不可控

> 模型幻觉是大语言模型（LLM）的核心缺陷之一，**指模型生成的内容看似合理、符合语法逻辑，但与事实完全不符，或包含无中生有的信息**。这些内容不是模型 “故意撒谎”，而是其基于训练数据的统计规律进行预测时，出现的 “编造” 或 “误判” 现象。

---

### 2. RAG 的标准流程？

1. 文档切分
2. 向量化
3. 向量存储
4. 相似度检索
5. 上下文拼接
6. LLM 生成

---

### 3. RAG 中 chunk 大小如何选择？
Chunk（文本块）大小的选择是**平衡检索精度、召回率和 LLM 上下文利用率**的核心，没有绝对统一的标准，需结合文本类型、业务场景、LLM 上下文窗口三个核心因素动态调整

| Chunk 大小 | 优势 | 劣势 | 适用场景 |
|------------|------|------|----------|
| 小 Chunk<br>（100-300 Token） | 1. 粒度细，与用户问题的匹配度更高，检索精度高；<br>2. 冗余信息少，LLM 聚焦核心内容，减少幻觉。 | 1. 易割裂文本语义（如拆分完整的段落 / 论点）；<br>2. 检索召回的 Chunk 数量多，可能超出 LLM 上下文窗口；<br>3. 向量化和存储开销略高。 | 结构化文本（如手册、FAQ、法律条文）、短问答场景、需要精准匹配的场景。 |
| 大 Chunk<br>（500-1000 Token） | 1. 保留完整语义（如完整的案例、技术原理、段落逻辑）；<br>2. 检索召回的 Chunk 数量少，节省 LLM 上下文空间；<br>3. 减少文本割裂导致的信息丢失。 | 1. 粒度粗，包含冗余信息，可能降低检索相关性；<br>2. 若 Chunk 过大，单个 Chunk 占满上下文窗口，无法召回多段相关内容。 | 非结构化文本（如论文、书籍章节、长文档）、需要完整上下文的场景（如技术原理解释）。 |
| 超大 Chunk<br>（>1000 Token） | 保留完整的长文本逻辑（如整章内容） | 1. 检索精度大幅下降；<br>2. 极易超出 LLM 上下文窗口；<br>3. 向量化效率低，语义表征模糊。 | 极少适用，仅用于超短文本的整体处理（如全文不足 1000 Token）。 |

#### chunk 大小如何选择
经验值：
- 300 ~ 800 tokens
- 需要结合：
    - 文档结构
    - 模型上下文长度

| 场景         | 文本类型                     | 推荐 Chunk 大小（Token） | 重叠率   |
|--------------|------------------------------|--------------------------|----------|
| 客服知识库   | FAQ、产品手册（结构化）| 100-300                  | 10%      |
| 技术文档     | API 文档、操作指南（半结构化） | 300-500                  | 10%-15%  |
| 论文 / 书籍  | 学术论文、书籍章节（非结构化） | 500-800                  | 15%-20%  |
| 新闻 / 博客  | 短文、新闻稿（半结构化）| 200-400                  | 10%      |
| 代码库       | 代码文件（结构化）| 200-300（按函数 / 类拆分） | 5%       |

> **token** 是大语言模型（LLM）**处理文本的最小语义单位**，**1 个 token 对应多少文字，全看分词器怎么拆**

> **召回率**（Recall） 是衡量检索系统查全能力的核心评估指标，它描述了检索结果中包含的 **“真实相关向量” 占 “所有真实相关向量” 的比例**。

> **准确率**则是检索结果里有多少是真正相关的

> **召回率和准确率往往此消彼长**


---

### 4. 向量检索常见问题有哪些？

| 问题现象 | 核心成因 | 解决方案 |
|----------|----------|----------|
| **召回率低**（漏检）：明明存在相关文本，检索结果却没有 | 1. 嵌入模型能力不足，无法将语义相似文本映射为相似向量<br>2. ANN 算法参数过于激进（如 HNSW 的 ef 过小），牺牲召回率换速度<br>3. Chunk 拆分破坏语义完整性（如拆分完整的问题 - 答案对） | 1. **升级嵌入模型**（如从 all-MiniLM-L6-v2 换为 bge-large-en-v1.5 或 OpenAI text-embedding-ada-002）<br>2. 调优 ANN 参数（增大 HNSW 的 ef/M、IVF 的 nprobe），必要时用 KNN 做基准对比<br>3. **按语义边界拆分 Chunk**（如 Markdown 标题、段落分隔符），设置 10%-20% 重叠率 |
| **准确率低（误检）**：检索结果大量与查询无关 | 1. 相似度度量方式选错（如文本语义检索用欧氏距离而非余弦相似度）<br>2. 向量维度冗余，引入噪声（如用 1536 维向量存储短文本）<br>3. 仅依赖向量相似度，未结合元数据过滤 | 1. 文本语义检索优先用 余弦相似度，数值向量（如图像特征）用欧氏距离，稀疏向量用内积<br>2. 用 PCA/SVD 降维，去除冗余维度<br>3. 采用混合检索（向量相似度 + 元数据过滤 + 关键词检索，如 LangChain 的 EnsembleRetriever） |
| **语义漂移**：通用嵌入模型在专业领域（医疗 / 法律）检索效果差 | 1. **嵌入模型训练数据是通用语料，未适配领域术语**<br>2. 领域文本的语义表达与通用语料差异大 | 1. 用领域语料微调嵌入模型（如 LoRA 微调 BERT）<br>2. 自定义领域分词词典，避免专业术语被拆分<br>3. 对长文本采用 “分段嵌入 + 聚合” 策略 |

---

### 5. 为什么常用 Milvus / FAISS？

**完美平衡了检索精度、性能和工程落地性**

#### 一、 核心优势：解决向量检索的核心痛点
####  1. 极致的检索性能（速度 + 精度的平衡）
向量检索的核心矛盾是 **“近似最近邻（ANN）算法的精度 vs 速度”**，Milvus/FAISS 是该领域的算法标杆：
- **FAISS**（Facebook AI 研究院开发）：ANN 算法集大成者，内置主流高效算法
  - Brute-force（暴力检索）：提供精确检索基准；
  - HNSW：当前性能最优的 ANN 算法（兼顾速度/精度），百万级数据查询延迟低至毫秒级；
  - IVF/IVF_PQ：适合亿级以上超大规模数据，支持量化压缩；
  - GPU 加速：FAISS-GPU 利用显卡并行计算，检索速度提升 **10~100 倍**。
- **Milvus**：基于 FAISS/Annoy/HNSW 做分布式封装，继承底层算法高性能，解决 FAISS 单机容量限制，**亿级数据分布式检索延迟可控制在 10~50ms**。

#### 2. 成熟的量化技术（降低存储/计算成本）
大规模向量检索的核心成本是存储和计算，二者均内置工业级量化方案，大幅压缩向量体积：

| 量化方式 | 压缩比 | FAISS 支持 | Milvus 支持 | 适用场景 |
|----------|--------|------------|-------------|----------|
| 标量量化（SQ） | 4 倍（float32→int8） | ✅ | ✅ | 中小规模数据，追求低成本且精度损失小 |
| 乘积量化（PQ） | 8~16 倍 | ✅ | ✅ | 亿级以上数据，容忍少量精度损失 |
| 倒排量化（IVF_PQ） | 16~32 倍 | ✅ | ✅ | 超大规模（十亿级）数据，极致压缩 |

**量化效果示例**：
1536 维 float32 向量（单条约 6KB），经 PQ 量化后仅需 0.375KB/条 → 1 亿条数据存储成本从 **580GB 降至 36GB**，且检索性能几乎无损失。

#### 3. 工程化适配性强（易集成、易扩展）
| 特性 | FAISS | Milvus |
|------|-------|--------|
| 架构特性 | 轻量级，纯 C++ 实现，提供 Python/Java 绑定 | 分布式架构，原生支持分片、副本，线性扩展至百台节点 |
| 集成能力 | 与 PyTorch/TensorFlow 等深度学习框架无缝衔接 | 支持与 LangChain/LLamaIndex 等 LLM 框架、PostgreSQL/MySQL 等数据库无缝集成 |
| 功能支持 | 直接嵌入应用，无额外部署成本；支持自定义距离函数、向量维度 | 全生命周期管理（增删改查、索引重建、冷热数据分离）；提供 RESTful/GRPC API，适配多开发语言 |
| 适配场景 | 原型验证、小规模场景、本地检索（客户端/边缘设备） | 生产级大规模检索、高并发场景、动态数据场景 |

#### 4. 开源免费 + 社区成熟
- **协议优势**：FAISS 采用 MIT 协议，Milvus 采用 Apache 2.0 协议 → 无商业许可成本，相比 Pinecone（商用）、Weaviate（小众）大幅降低使用门槛；
- **社区保障**：
  - FAISS：向量检索算法的**基准工具**，几乎所有相关论文/方案都会对比 FAISS 性能；
  - Milvus：由 Zilliz 商业化公司维护，文档、教程、问题解答体系完善，生产级支持有保障。

#### 二、 适用场景：覆盖从原型到生产的全流程
| 工具 | 核心定位 | 适用场景 | 典型数据规模 |
|------|----------|----------|--------------|
| FAISS | 单机向量检索库 | 1. 算法原型验证（快速测试不同 ANN 算法效果）<br>2. 小规模向量检索（百万级以内）<br>3. 嵌入应用的本地检索（如客户端/边缘设备） | 百万级以下，单机可承载 |
| Milvus | 分布式向量数据库 | 1. 生产级大规模向量检索（亿级以上）<br>2. 高并发场景（QPS 1000+）<br>3. 动态数据场景（需增量更新、事务一致性） | 亿级～十亿级，分布式集群承载 |

#### 三、对比其他工具

| 替代工具                | 核心劣势                                  | Milvus/FAISS 的优势                          |
|-------------------------|-------------------------------------------|---------------------------------------------|
| Pinecone（商用向量数据库）| 成本高（按向量存储量 / 查询次数收费）、私有化部署复杂 | 开源免费，私有化部署灵活，无按量付费成本     |
| pgvector（PostgreSQL 扩展）| 性能有限（百万级以上检索延迟显著升高）、分布式能力弱 | 检索性能高，Milvus 支持分布式扩展，FAISS 单机性能更优 |
| Annoy/HNSWlib（轻量库）  | 算法单一（仅支持 HNSW）、无量化 / 分布式能力 | 算法丰富，支持多种量化方案，Milvus 补充分布式能力 |
| Elasticsearch（向量插件） | 向量检索并非核心功能，亿级数据性能差、成本高 | 专为向量检索优化，存储 / 计算效率更高         |

#### Milvus 和 FAISS 成为主流的核心逻辑是：
- **技术层面**：算法先进（覆盖全品类 ANN 算法）、性能极致（GPU / 分布式加速）、成本可控（量化压缩）；
- **工程层面**：FAISS 轻量易集成（适合原型），Milvus 分布式易扩展（适合生产），覆盖全流程；
- **生态层面**：开源免费、社区成熟，与 LLM/RAG 等主流场景无缝兼容。

简单来说：做向量检索，FAISS 是 **“算法基准”**，Milvus 是 **“生产落地”** 的首选，二者互补，几乎能覆盖所有向量检索场景。

---

## 三、Memory & 上下文管理

### 1. LangChain 的 Memory 是干什么的？

LangChain 的 Memory（记忆）组件是专门解决**大模型单次调用无上下文**问题的核心模块，本质是**对话历史的管理器**—— 它能持久化、标准化、高效地存储和复用多轮对话信息，让 LLM 应用（如聊天机器人、智能助手）拥有 “短期记忆”，实现自然的多轮交互，而非每次对话都从零开始。

- **存储对话历史**：记录每一轮的「用户输入」和「模型输出」（部分支持记录系统提示）；
- **加载对话历史**：在新轮次请求时，自动将历史信息注入提示词，传递给 LLM；
- **优化历史内容**：对长对话做摘要、截断，平衡上下文完整性和 Token 消耗；
- **持久化存储**：支持将对话历史存到外部数据库（Redis/SQL/MongoDB），适配多用户、长会话场景。

---

### 2. 常见 Memory 类型？

| Memory 类型 | 核心特点 | 适用场景 | 典型实现 |
|-------------|----------|----------|----------|
| 临时内存型 | 对话历史存于内存，轻量但重启后丢失 | 短对话、单用户、原型验证 | ConversationBufferMemory（完整存储）<br>ConversationBufferWindowMemory（仅存最近 N 轮） |
| 摘要压缩型 | 对长对话生成摘要，减少 Token 占用 | 长对话、上下文窗口有限（如 GPT-3.5） | ConversationSummaryMemory（全量摘要）<br>ConversationSummaryBufferMemory（摘要 + 窗口） |
| 结构化记忆 | 提取对话中的实体 / 关键信息（如用户姓名、偏好），结构化存储 | 需精准复用用户关键信息的场景（如客服、助手） | EntityMemory（提取实体）<br>KGMemory（知识图谱式存储） |
| 持久化记忆 | 对话历史存到外部数据库，支持多用户、跨会话复用 | 生产环境、多用户、长会话 | RedisChatMessageHistory（Redis 存储）<br>SQLChatMessageHistory（SQL 数据库）<br>MongoDBChatMessageHistory（MongoDB 存储） |

---

### 3. 长对话如何解决 token 爆炸？

| 策略类型       | 核心原理                                                                 | 适用场景                                   | 典型实现（LangChain）                          |
|----------------|--------------------------------------------------------------------------|--------------------------------------------|------------------------------------------------|
| **窗口截断**       | 只保留最近 N 轮对话，丢弃更早的历史                                     | 对话逻辑关联性弱、近期内容更重要           | ConversationBufferWindowMemory                 |
| **摘要压缩**       | 用 LLM 将长对话历史浓缩为一段摘要，替代原始历史                         | 对话逻辑连贯、有明确主题（如咨询、分析）| ConversationSummaryMemory                      |
| **混合策略**       | 窗口截断 + 摘要压缩，保留最近 N 轮原始对话 + 更早历史的摘要             | 长对话且前后逻辑强关联                     | ConversationSummaryBufferMemory                |
| **关键信息提取**   | 仅提取对话中的核心实体 / 意图 / 结论，丢弃冗余话术                       | 任务型对话（如客服、指令执行）| EntityMemory/ 自定义提取器                     |

---

### 4. 为什么不能无限拼历史对话？

- token 成本高
- 模型注意力下降
- 回答质量下降

---

## 四、Tool Calling & Agent（高频）

### 1. 什么是 Tool Calling？

Tool Calling（工具调用）是**大语言模型（LLM）突破自身能力边界，与外部工具 / 系统交互的核心机制**—— 简单来说，就是让 LLM 不仅能 “生成文本”，还能像人一样 “思考是否需要工具→选择合适工具→传入正确参数→执行工具→用工具结果生成最终回答”

让模型：
- **按 JSON 格式输出**
- 调用外部系统能力

---

### 2. Tool Calling 如何防止模型乱调用？

- System Prompt 约束
- Tool 白名单（按需暴露）
- 参数 Schema 校验

---

### 3. Agent 和 Chain 的区别？

| 组件  | 核心定义                                                                 | 本质                                                                 | 核心关键词                     |
|-------|--------------------------------------------------------------------------|----------------------------------------------------------------------|--------------------------------|
| Chain | 将多个组件（Prompt、LLM、Retriever、Memory 等）按固定顺序 / 逻辑串联，形成可复用的工作流 | 「自动化流水线」：输入数据按预设路径流经各个组件，输出结果             | 固定流程、无决策、可预测、易调试 |
| Agent | 以 LLM 为核心的「决策器」，结合工具（Tool）和记忆（Memory），能自主推理、选择工具、调整步骤完成复杂任务 | 「智能助手」：根据用户指令，自主判断 “是否调用工具→调用哪个工具→传入什么参数→是否需要多轮调用” | 自主决策、工具调用、动态流程、适配复杂任务 |

| 对比 | Chain | Agent |
|----|----|----|
| 执行路径 | 固定 | 动态 |
| 决策能力 | 无 | 有 |
| 风险 | 低 | 高 |

---

### 4. Agent 的风险点有哪些？

Agent 作为 LangChain 中具备自主决策能力的核心组件，虽然极大扩展了 LLM 应用的边界，但由于其**动态性、不可预测性**，在落地过程中会面临一系列风险。这些风险主要集中在 **决策逻辑、工具调用、系统安全、成本性能** 四个维度

| 风险类型       | 核心防控手段                                                                                                                                                                                                 |
|----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **决策逻辑风险**   | 1. 精细化工具描述（明确功能边界、参数约束）<br>2. 加入少样本示例引导正确决策<br>3. 设置最大重试次数，避免死循环                                                                                              |
| **工具调用风险**   | 1. 用 StructuredTool 做参数校验，白名单限制参数取值<br>2. 给工具设置调用频率阈值和权限白名单<br>3. 对工具返回结果做格式和内容校验                                                                              |
| **安全合规风险**   | 1. 敏感数据脱敏（如替换身份证号、手机号中的部分字符）<br>2. 工具返回内容做合规审核（关键词过滤、违规内容识别）<br>3. 工具调用设置超时和熔断机制                                                                 |
| **成本性能风险**   | 1. 设置 LLM 调用次数上限和 Token 预算<br>2. 对高频工具调用结果做缓存<br>3. 支持工具并行调用，减少串行等待时间                                                                                                |

---

### 5. 如何控制 Agent 行为？

- 限制 tool 数量
- 限制调用次数
- 明确 System Prompt

---

## 五、LangChain 工程化

### 1. LangChain 如何与后端服务集成？

LangChain 本质是 “LLM 应用的开发框架”，后端服务则负责对外提供接口、处理请求、管控资源、保障稳定性。二者集成的核心逻辑如下：

1. **封装 LangChain 逻辑**：将 Chain/Agent/Retrieval 等核心逻辑封装为独立的函数 / 类，解耦业务逻辑与接口层；
2. **后端框架暴露 API**：用 FastAPI/Flask 等框架编写接口，接收前端 / 客户端请求，调用封装好的 LangChain 模块；
3. **工程化增强**：添加鉴权、日志、缓存、异常处理，适配后端部署（容器化、集群化）。

---

### 2. LangChain 如何支持多用户并发？

- **隔离**：通过 session_id 隔离用户 Memory、上下文、工具调用状态；
- **复用**：全局单例化 LLM / 向量库等无状态组件，减少资源消耗；
- **异步**：使用异步调用 + 异步框架 + 任务队列，提升并发处理能力；
- **管控**：限流、熔断、超时控制，防止服务过载；
- **缓存**：减少重复计算，提升响应速度。

---

### 3. 如何保证 AI 应用的安全性？

- Prompt Injection 防护
- SQL Tool 限制
- 输出校验

---

### 4. 如何防止 AI 执行危险 SQL？

- 只允许 SELECT
- SQL AST 校验
- 只读账号

---

## 六、真实面试场景题

### 1. RAG 命中率低，你如何排查？

排查路径：
- embedding 模型
- chunk 切分
- 相似度阈值
- 是否需要 re-rank

---

### 2. 模型回答经常“胡说”，怎么办？

- 强化 System Prompt
- 引入 RAG
- 输出格式校验

---

### 3. 为什么 Agent 不适合所有场景？

- 不可控
- 成本高
- Debug 困难

---


### 4、你怎么理解 RAG？为什么不用直接让模型回答？

- RAG 的本质是 “把**不可控的大模型**，变成可控的业务系统”。


### 5、如何防止 AI 乱查数据库、乱改数据？
- Prompt 约束  
> 明确只允许 SELECT

- Tool 层约束  
>SQL Parser 校验

- 权限隔离  
> 只读账号

### 6、你们的 AI 是如何处理“长对话”的？

- 窗口记忆（最近 N 条）  
- 旧对话 → 总结成语义记忆

















